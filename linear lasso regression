import os
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn import tree
from sklearn import model_selection
import math
from sklearn import metrics
#import io
from sklearn.tree import DecisionTreeRegressor
from sklearn import linear_model
#import pydot
from sklearn.ensemble import RandomForestRegressor
from sklearn import preprocessing
from sklearn_pandas import DataFrameMapper
from sklearn import decomposition
from sklearn import ensemble
#from mlxtend.regressor import StackingRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn import neighbors
#import xgboost as xgb
#returns current working directory
os.getcwd()
#changes working directory
os.chdir('C:/Projects/DataScience/House_pricing')
house_train = pd.read_csv("train_house.csv")
house_test = pd.read_csv("test_house.csv")
#house_train = pd.read_csv("train_house.csv")
#house_test = pd.read_csv("test_house.csv")
house_train.shape
house_train.info()


house_data = pd.concat([house_train, house_test],ignore_index=True)
house_data.drop(["Id","SalePrice","Utilities"], 1, inplace=True)
house_data.shape
house_data.info()

#convert numerical columns to categorical type              
house_data['MSSubClass'] = house_data['MSSubClass'].astype('category')


#ordinal_features1 = [col for col in house_train if 'TA' in list(house_train[col])]
house_mszoning = {None: 0, "C": 4, "FV": 8, "RH": 6, "RL": 7, "RP": 3,"RM":5,"A":2,"I":1}
house_data["MSZoning"] = house_data["MSZoning"].map(house_mszoning)

house_pavedDrive = {None: 0, "Y": 3, "P": 2, "N": 1}
house_data["PavedDrive"] = house_data["PavedDrive"].map(house_pavedDrive)

house_garageFinish = {None: 0, "Fin": 3, "RFn": 2, "Unf": 1,"NA":0}
house_data["GarageFinish"] = house_data["GarageFinish"].map(house_garageFinish)

house_electrical = {None: 0, "SBrkr": 5, "FuseA": 4, "FuseF": 3,"FuseP":2,"Mix":1}
house_data["Electrical"] = house_data["Electrical"].map(house_electrical)


#house_CentralAir = {None: 0, "Y": 2, "N": 1}
#house_data["CentralAir"] = house_data["CentralAir"].map(house_CentralAir)

house_Heating = {None: 0, "GasA": 6, "GasW": 5, "OthW": 4,"Wall":3,"Garv":2,"Floor":1}
house_data["Heating"] = house_data["Heating"].map(house_Heating)



#convert categorical columns to numeric type
ordinal_features1 = ["ExterQual", "ExterCond", "BsmtQual", "BsmtCond", "GarageQual", "GarageCond", "PoolQC", "FireplaceQu", "KitchenQual", "HeatingQC"]
#ordinal_features1 = [col for col in house_train if 'TA' in list(house_train[col])]
quality_dict = {None: 0, "Po": 1, "Fa": 2, "TA": 3, "Gd": 4, "Ex": 5}
for feature in ordinal_features1:
    null_idx = house_data[feature].isnull()
    house_data.loc[null_idx, feature] = None 
    house_data[feature] = house_data[feature].map(quality_dict)
    
house_data["Total_quality"] = (house_data["OverallQual"]+house_data["OverallCond"])/2
house_data['Total_quality'] = house_data['Total_quality'].astype('category')


##Plotinng
#sns.boxplot(x = 'GarageType', y = 'SalePrice',  data = house_train)
#plt.xticks(rotation=45)


#sns.boxplot(x ='YearBuilt', y ='SalePrice',  data = house_train)

def category_types(column):
    return house_data[column].value_counts()

def impute_category(column, value):
    house_data.loc[house_data[column].isnull(),column] = value
                   
def impute_continous(column):
    house_data.loc[house_data[column].isnull(),column] = house_data[column].mean()
    
category_types('GarageYrBlt')
impute_continous('BsmtFinSF1')
impute_continous('BsmtFinSF2')
impute_continous('MSZoning')
impute_continous('BsmtFullBath')
impute_continous('BsmtHalfBath')
impute_continous('BsmtUnfSF')
impute_continous('GarageArea')
impute_continous('GarageCars')
impute_continous('TotalBsmtSF')
impute_continous('MasVnrArea')

impute_category('Exterior1st','None')
impute_category('Exterior2nd','None')
impute_category('Functional','None')
impute_category('SaleType','None')
#impute_category('Utilities','None')
impute_category('Heating','None')
impute_category('MasVnrType','None')
#impute_category('BsmtFinType1','NA')
#impute_category('BsmtFinType2','NA')
#handle missing data columns
total_missing = house_data.isnull().sum()
to_delete = total_missing[total_missing>0]
type(to_delete)
house_data.drop(list(to_delete.index), axis=1, inplace=True)
house_data.shape
house_data.drop(['YearBuilt','YearRemodAdd'], axis=1, inplace=True)


numeric_cols = house_data.select_dtypes(include=['number']).columns
cat_cols = house_data.select_dtypes(exclude = ['number']).columns

house_data1 = pd.get_dummies(house_data, columns=cat_cols)
house_data1.shape

house_train1 = house_data1[:house_train.shape[0]]
house_test1 = house_data1[house_train.shape[0]:]
house_train['log_sale_price'] = np.log(house_train['SalePrice'])

X_train = house_train1
y_train = house_train['log_sale_price']

corr = house_data.corr()



### Most 
def correlation(dataset,testdataset, threshold):
    col_corr = set() # Set of all the names of deleted columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if corr_matrix.iloc[i, j] >= threshold:
                colname = corr_matrix.columns[i] # getting the name of column
                col_corr.add(colname)
                print(colname,corr_matrix.iloc[i, j])
                if colname in dataset.columns:
                    del dataset[colname] # deleting the column from the dataset
                    del testdataset[colname]

correlation(X_train,house_test1, 0.88)

#### Zero variance columns are removed
variance = X_train.var()
i=0
for k in variance.iteritems():
    if(k[1]<0.005):
        if (k[0] in X_train.columns)&(k[0] in house_test1.columns):
            print(k[0],"val",k[1])
            i=i+1
            del X_train[k[0]]
            del house_test1[k[0]]
print("Columns to be deleted",i)




base_models = [
     RandomForestRegressor(random_state=2017,max_depth=5,min_samples_split=5),
     GradientBoostingRegressor(random_state=2017,max_depth=4,min_samples_split=10),
    # neighbors.KNeighborsRegressor(),
     #xgb.XGBRegressor(colsample_bytree=0.2, gamma=0.0, 
      #                       learning_rate=0.05, max_depth=6, 
       #                      min_child_weight=1.5, n_estimators=7200,
        #                     reg_alpha=0.9, reg_lambda=0.6,
         #                    subsample=0.2,seed=42, silent=1,
          #                   random_state =7),
     #DecisionTreeRegressor(max_depth=4)
     ]
    
#dt_estimator = linear_model.Lasso()
##rf_estimator = ensemble.RandomForestRegressor()
#rf_grid = {'alpha':[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09]}
#rf_grid_estimator = model_selection.GridSearchCV(dt_estimator,rf_grid, cv=10, n_jobs=10)
#rf_grid_estimator.fit(X_train, y_train)
##print("CvScore",rf_grid_estimator.best_score_)
##print("Best estimator",rf_grid_estimator.best_estimator_)
#print("Best estimator",rf_grid_estimator.get_params())


#lassocv = linear_model.LassoCV(alphas = [1, 0.1, 0.001, 0.0005])
#lasso_param_grid= dict(max_iter = range(100,1250,50))
#lasso_grid_estimator = model_selection.GridSearchCV(lassocv,lasso_param_grid,scoring="neg_mean_squared_error",n_jobs = 5,verbose = 2,cv =10)
#lasso_grid_estimator.fit(X_train,y_train)
#print(lasso_grid_estimator.best_params_)
#print(lasso_grid_estimator.best_estimator_.feature_importances_)
#print(math.sqrt(-lasso_grid_estimator.best_score_))
#print(math.sqrt(-lasso_grid_estimator.score(X_train,y_train)))

lassocv = linear_model.LassoCV(alphas = [0.001])
lasso_param_grid= dict(max_iter = range(100,1500,100))
lasso_grid_estimator = model_selection.GridSearchCV(lassocv,lasso_param_grid,scoring="neg_mean_squared_error",n_jobs = 5,verbose = 2,cv =10)
lasso_grid_estimator.fit(X_train,y_train)
print(lasso_grid_estimator.best_params_)
paramaters =lasso_grid_estimator.get_params(deep=True)
lasso_grid_estimator.feature_importances_
print(lasso_grid_estimator.best_estimator_.feature_importances_)
print(math.sqrt(-lasso_grid_estimator.best_score_))
print(math.sqrt(-lasso_grid_estimator.score(X_train,y_train)))


#dt_estimator = StackingRegressor(regressors=base_models,meta_regressor=ensemble.RandomForestRegressor(random_state=2017,max_depth=4,min_samples_split=10))
#evaluate using r^2
print("r2 deviation",model_selection.cross_val_score(lasso_grid_estimator, X_train, y_train, cv=10,scoring="r2").mean())

#evaluate using rmse - 1
res = model_selection.cross_val_score(lasso_grid_estimator, X_train, y_train, cv=10,scoring="neg_mean_squared_error").mean()
print("RMSE Default",math.sqrt(-res))

#evaluate using rmse - 2
def rmse(y_original,  y_pred):
   return math.sqrt(metrics.mean_squared_error(y_original, y_pred))
      
res = model_selection.cross_val_score(lasso_grid_estimator, X_train, y_train,cv=10,scoring=metrics.make_scorer(rmse),verbose = 2).mean()
print("RMSE Custom" , res)

### Most 




#mapper = DataFrameMapper([(X_train.columns, preprocessing.StandardScaler())])
#scaled_features = mapper.fit_transform(X_train)
#type(scaled_features)
#X_train_999 = pd.DataFrame(scaled_features, columns=X_train.columns)

#pca = decomposition.PCA(n_components=84)
#pca.fit(X_train_999)
#explainedVariance = pca.explained_variance_
#varianceRatio = pca.explained_variance_ratio_
#varianceCumSum = pca.explained_variance_ratio_.cumsum()
#X_train1 = pd.DataFrame(pca.transform(X_train_999))

#rf_estimator = ensemble.RandomForestRegressor()
#rf_grid = {'n_estimators':list(range(100,500,100)),'max_depth':[4,5,6,7,8],'min_samples_split':[4,5,6,7,8,9,10]}
#rf_grid_estimator = model_selection.GridSearchCV(rf_estimator,rf_grid, cv=10, n_jobs=10)
#rf_grid_estimator.fit(X_train, y_train)
#print("CvScore",rf_grid_estimator.best_score_)
#rf_grid_estimator.best_estimator_

#dt_estimator = RandomForestRegressor(random_state=2017,max_depth=8,min_samples_split=5)



lasso_grid_estimator.fit(X_train, y_train)
#v = dt_estimator.feature_importances_
#type(v)

#dot_data = io.StringIO() 
#tree.export_graphviz(dt_estimator, out_file = dot_data, feature_names = X_train.columns)
#graph = pydot.graph_from_dot_data(dot_data.getvalue())[0] 
#graph.write_pdf("reg-tree1.pdf")

#mapper = DataFrameMapper([(titanic_test1.columns, preprocessing.StandardScaler())])
#scaled_features = mapper.fit_transform(titanic_test1)
#type(scaled_features)
#titanic_test_999 = pd.DataFrame(scaled_features, columns=titanic_test1.columns)
#
#
#titanic_test2 = pd.DataFrame(pca.transform(titanic_test_999))


X_test = house_test1
log_sales_price = lasso_grid_estimator.predict(X_test)
house_test['SalePrice'] = np.exp(log_sales_price)

#house_test['Survived'] = dt_estimator.predict(house_test)
house_test.to_csv("submission.csv", columns=['Id','SalePrice'], index=False)






#house_test.shape
#house_test.info()
#
#house_data = pd.concat([house_train, house_test],ignore_index=True)
#
##house_data.drop(house_data[(house_data['GrLivArea']>4000) & (house_data['SalePrice']<300000)].index, inplace=True)
#house_data.drop(["Id","SalePrice","Utilities"], 1, inplace=True)
#house_data.shape
#house_data.info()
